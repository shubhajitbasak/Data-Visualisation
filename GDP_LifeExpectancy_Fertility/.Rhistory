setwd("D:/Ratul/MyGit/Data-Visualisation/GDP_LifeExpectancy_Fertility")
unlink('D:/Ratul/MyGit/Data-Visualisation/TextAnalytics/TextAnalytics_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message = FALSE)
library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(ggdendro)
library(dplyr)
library(cluster)
library(HSAUR)
library(fpc)
library(skmeans)
library(plyr)
library(gplots)
library(stats)
library(philentropy)
library(networkD3)
library(ape)
library(RColorBrewer)
library(wesanderson)
library(wordcloud2)
library(treemapify)
library(knitr)
library(kableExtra)
setwd("D:/Ratul/MyGit/Data-Visualisation/TextAnalytics/")
corpus <- VCorpus(DirSource("corpus", recursive = TRUE, encoding = "UTF-8"),
readerControl = list(language = "eng"))
# Cleaning the corpus..
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "/.")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace, "\\|")
# Converting all text to lower case. removing stop words and punctuations
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removePunctuation)
# Remove numbers, letters and stemming
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, c(letters))
corpus <- tm_map(corpus, stemDocument)
corpus.dtm <- DocumentTermMatrix(corpus,
control = list(
weighting = function(x)
weightTfIdf(x, normalize = TRUE)))
sparsity_threshold = 0.9995
corpus.dtm<-removeSparseTerms(corpus.dtm, sparsity_threshold)
corpus.dtm.mat <- corpus.dtm %>% as.matrix()
# remove any zero rows
corpus.dtm.mat <- corpus.dtm.mat[rowSums(corpus.dtm.mat^2) !=0,]
set.seed(62)
percent = 40
sample_size = nrow(corpus.dtm.mat) * percent/100
corpus.dtm.mat.sample <- corpus.dtm.mat[sample(1:nrow(corpus.dtm.mat),
sample_size, replace=FALSE),]
corpus.tdm <- TermDocumentMatrix(corpus, control =
list(weighting = function(x) weightTf(x)))
corpus.tdm<-removeSparseTerms(corpus.tdm, 0.999)
corpus.tdm.sample <- corpus.tdm[, rownames(corpus.dtm.mat.sample)]
corpus.tdm.sample.mat <- corpus.tdm.sample %>% as.matrix()
cost_df <- data.frame()
#run kmeans for all clusters up to 100
for(i in 1:30){
#Run kmeans for each level of i, allowing up to 100 iterations for convergence
kmeans<- kmeans(x=corpus.dtm.mat.sample, centers=i, iter.max=100)
#Combine cluster number and cost together, write to df
cost_df<- rbind(cost_df, cbind(i, kmeans$tot.withinss))
}
